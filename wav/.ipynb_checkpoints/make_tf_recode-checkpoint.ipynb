{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_NUM_SHARDS = 5\n",
    "_DATA_DIR = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filenames_and_classes(dataset_dir, train_val):\n",
    "    wave_root = os.path.join(dataset_dir, train_val)\n",
    "    directories = []\n",
    "    class_names = []\n",
    "#     print (wave_root)\n",
    "#     print (os.listdir(wave_root))\n",
    "    for filename in os.listdir(wave_root):\n",
    "        path = os.path.join(wave_root, filename)\n",
    "        if os.path.isdir(path):\n",
    "            directories.append(path)\n",
    "            class_names.append(filename)\n",
    "\n",
    "    photo_filenames = []\n",
    "#     print (directories)\n",
    "#     print (class_names)\n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "#             print (filename)\n",
    "            path = os.path.join(directory, filename)\n",
    "            photo_filenames.append(path)\n",
    "\n",
    "    return photo_filenames, sorted(class_names)\n",
    "\n",
    "wave_filenames, class_names = get_filenames_and_classes(_DATA_DIR, \"9th Wonder Kit/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./9th Wonder Kit/Percussions/Tab_04.wav'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave_filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Kicks': 3, 'Shakers': 5, 'Snares': 6, 'Percussions': 4, 'Cymbals': 1, 'Claps': 0, 'Hi-Hats': 2}\n",
      "file len:  521 num per shard:  105\n",
      ">> Converting image 1/521 shard 0(9923,)\n",
      "(128, 20)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "array([[-26.15721425, -25.19977996, -28.86151974, ..., -80.        ,\n        -80.        , -80.      has type numpy.ndarray, but expected one of: bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-2e771365cef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclass_names_to_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mconvert_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwave_filenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names_to_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_DATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-2e771365cef5>\u001b[0m in \u001b[0;36mconvert_dataset\u001b[0;34m(split_name, filenames, class_names_to_ids, dataset_dir)\u001b[0m\n\u001b[1;32m     86\u001b[0m                         \u001b[0;31m#make tensorflow record object and write\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                         example = wave_to_tfexample(\n\u001b[0;32m---> 88\u001b[0;31m                             log_S, b'array', class_id)\n\u001b[0m\u001b[1;32m     89\u001b[0m                         \u001b[0mtfrecord_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-2e771365cef5>\u001b[0m in \u001b[0;36mwave_to_tfexample\u001b[0;34m(wave_arr, wave_format, class_id)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwave_to_tfexample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwave_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwave_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     return tf.train.Example(features=tf.train.Features(feature={\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0;34m'./9th_recode/encoded'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwave_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m       \u001b[0;34m'./9th_recode/format'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwave_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0;34m'./9th_recode/class/label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint64_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-2e771365cef5>\u001b[0m in \u001b[0;36mbytes_feature\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# # Creates a TensorFlow Record Feature with value as a byte array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbytes_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# #define tesnorflow record format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: array([[-26.15721425, -25.19977996, -28.86151974, ..., -80.        ,\n        -80.        , -80.      has type numpy.ndarray, but expected one of: bytes"
     ]
    }
   ],
   "source": [
    "class WaveReader(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        record_defaults = [tf.constant([], dtype=tf.float32)]\n",
    "        # Initializes function that decodes 1 channel png data.\n",
    "        self._decode_csv_data = tf.placeholder(dtype=tf.string)\n",
    "        self._decode_csv = tf.decode_csv(self._decode_csv_data, record_defaults=record_defaults)\n",
    "\n",
    "    def read_wave_dims(self, sess, wave_arr):\n",
    "        wave = self._decode_csv(sess, wave_arr)\n",
    "        return wave.shape\n",
    "    #decode string image data to int image data\n",
    "    def decode_png(self, sess, wave_arr):\n",
    "        wave = sess.run(self._decode_csv,\n",
    "                         feed_dict={self._decode_csv_data: wave_arr})\n",
    "        return wave.shape\n",
    "\n",
    "# make tensorflow record file name\n",
    "def get_dataset_filename(dataset_dir, split_name, shard_id):\n",
    "    output_filename = 'wave_%s_%05d-of-%05d.tfrecord' % (\n",
    "        split_name, shard_id, _NUM_SHARDS)\n",
    "    return os.path.join(dataset_dir, output_filename)\n",
    "\n",
    "# Creates a TensorFlow Record Feature with value as a 64 bit integer.\n",
    "def int64_feature(values):\n",
    "    # if value is not tuple or list, make value to list\n",
    "    if not isinstance(values, (tuple, list)):\n",
    "        values = [values]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n",
    "\n",
    "# # Creates a TensorFlow Record Feature with value as a byte array\n",
    "def bytes_feature(values):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n",
    "\n",
    "# #define tesnorflow record format\n",
    "def wave_to_tfexample(wave_arr, wave_format, class_id):\n",
    "    return tf.train.Example(features=tf.train.Features(feature={\n",
    "      './9th_recode/encoded': bytes_feature(wave_arr),\n",
    "      './9th_recode/format': bytes_feature(wave_format),\n",
    "      './9th_recode/class/label': int64_feature(class_id),\n",
    "#       './9th_recode/height': int64_feature(height),\n",
    "#       './9th_recode/width': int64_feature(width),\n",
    "    }))\n",
    "\n",
    "def convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir):\n",
    "    assert split_name in ['train', 'validation']\n",
    "    \n",
    "    num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n",
    "\n",
    "    print (\"file len: \",len(filenames), \"num per shard: \", num_per_shard)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        wave_reader = WaveReader()\n",
    "\n",
    "        with tf.Session('') as sess:\n",
    "\n",
    "            for shard_id in range(_NUM_SHARDS):\n",
    "                output_filename = get_dataset_filename(\n",
    "                    dataset_dir, split_name, shard_id)\n",
    "\n",
    "                #print (output_filename)\n",
    "                \n",
    "                #write tensorflow record file\n",
    "                #create tfrecord_write\n",
    "                with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n",
    "                    start_ndx = shard_id * num_per_shard\n",
    "                    end_ndx = min((shard_id + 1) * num_per_shard, len(filenames))\n",
    "                    for i in range(start_ndx, end_ndx):\n",
    "                        sys.stdout.write('\\r>> Converting image %d/%d shard %d' % (\n",
    "                            i + 1, len(filenames), shard_id))\n",
    "                        sys.stdout.flush()\n",
    "\n",
    "                        # Read the filename:\n",
    "                        wave_arr, sr = librosa.load(filenames[i])\n",
    "                        print(wave_arr.shape)\n",
    "                        S = librosa.feature.melspectrogram(wave_arr, sr=sr, n_mels=128)\n",
    "                        log_S = librosa.logamplitude(S, ref_power=np.max)\n",
    "#                         wave_arr = tf.gfile.FastGFile(filenames[i], 'rb').read()\n",
    "#                         height, width = wave_reader.read_wave_dims(sess, log_S)\n",
    "#                         print(height)\n",
    "#                         print(width)\n",
    "                        print(log_S.shape)\n",
    "                        class_name = os.path.basename(os.path.dirname(filenames[i]))\n",
    "                        class_id = class_names_to_ids[class_name]\n",
    "                        \n",
    "                        #make tensorflow record object and write\n",
    "                        example = wave_to_tfexample(\n",
    "                            log_S, b'array', class_id)\n",
    "                        tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "    sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "random.shuffle(wave_filenames)\n",
    "# random.shuffle(validation_filenames)\n",
    "\n",
    "class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n",
    "print (class_names_to_ids)\n",
    "\n",
    "convert_dataset('train', wave_filenames, class_names_to_ids, _DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
